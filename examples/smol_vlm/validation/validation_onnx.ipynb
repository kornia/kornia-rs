{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61697376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "from transformers.image_utils import load_image\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "\n",
    "import torch, random, numpy as np\n",
    "from transformers import set_seed\n",
    "\n",
    "def set_all_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    set_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_all_seeds(9)\n",
    "\n",
    "from safetensors import safe_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50c69e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM-256M-Instruct\")\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            # {\"type\": \"image\"},\n",
    "            # {\"type\": \"text\", \"text\": \"Can you describe the image?\"}\n",
    "            {\"type\": \"text\", \"text\": \"What is life?\"}\n",
    "            # {\"type\": \"text\", \"text\": \"A real-valued function f defined on the real line is called an even function if f(-t) = f(t) for each real number t. Prove that the set of even functions defined on the real line with the operations of addition and scalar multiplication defined in Example 3 is a vector space.\"}\n",
    "        ]\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "815b0101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.idefics3.modeling_idefics3.Idefics3ForConditionalGeneration"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize model directly on CUDA without Flash Attention\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    \"HuggingFaceTB/SmolVLM-256M-Instruct\",\n",
    "    torch_dtype=torch.float32,\n",
    "    # _attn_implementation=\"flash_attention_2\",  # Commented out Flash Attention\n",
    "    device_map=\"cpu\",\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "\n",
    "embeddings = {}\n",
    "counter_token_pos = -1\n",
    "\n",
    "def hook_fn(name, initial_layer: bool = False):\n",
    "    def hook(module, input, output):\n",
    "        global counter_token_pos\n",
    "        if initial_layer:\n",
    "            counter_token_pos += 1\n",
    "        \n",
    "        if isinstance(output, tuple):\n",
    "            output = output[0]\n",
    "        elif isinstance(output, torch.Tensor):\n",
    "            pass\n",
    "        else:\n",
    "            print(\"Hook unknown type!!!\", name, type(output))\n",
    "\n",
    "        embeddings[name+f\"_{counter_token_pos}\"] = output.detach().cpu()\n",
    "\n",
    "    return hook\n",
    "\n",
    "# Register hooks for different layers\n",
    "model.get_input_embeddings().register_forward_hook(hook_fn(\"input_embeddings\", initial_layer=True))\n",
    "for i in range(24):\n",
    "    model.model.text_model.layers[i].input_layernorm.register_forward_hook(hook_fn(f\"input_layernorm_d{i}\"))\n",
    "    model.model.text_model.layers[i].self_attn.register_forward_hook(hook_fn(f\"self_attn_d{i}\"))\n",
    "    model.model.text_model.layers[i].self_attn.k_proj.register_forward_hook(hook_fn(f\"self_attn_k_proj_d{i}\"))\n",
    "    model.model.text_model.layers[i].self_attn.v_proj.register_forward_hook(hook_fn(f\"self_attn_v_proj_d{i}\"))\n",
    "    model.model.text_model.layers[i].self_attn.q_proj.register_forward_hook(hook_fn(f\"self_attn_q_proj_d{i}\"))\n",
    "    model.model.text_model.layers[i].self_attn.o_proj.register_forward_hook(hook_fn(f\"self_attn_o_proj_d{i}\"))\n",
    "    model.model.text_model.layers[i].post_attention_layernorm.register_forward_hook(hook_fn(f\"post_layernorm_d{i}\"))\n",
    "    # model.model.text_model.layers[i].mlp.register_forward_hook(hook_fn(f\"mlp_d{i}\"))\n",
    "    model.model.text_model.layers[i].mlp.gate_proj.register_forward_hook(hook_fn(f\"mlp_gate_proj_d{i}\"))\n",
    "    model.model.text_model.layers[i].mlp.up_proj.register_forward_hook(hook_fn(f\"mlp_up_proj_d{i}\"))\n",
    "    model.model.text_model.layers[i].mlp.down_proj.register_forward_hook(hook_fn(f\"mlp_down_proj_d{i}\"))\n",
    "    model.model.text_model.layers[i].mlp.act_fn.register_forward_hook(hook_fn(f\"mlp_act_fn_d{i}\"))\n",
    "    model.model.text_model.layers[i].register_forward_hook(hook_fn(f\"layers_d{i}\"))\n",
    "\n",
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "046cbb62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Idefics3ForConditionalGeneration(\n",
       "  (model): Idefics3Model(\n",
       "    (vision_model): Idefics3VisionTransformer(\n",
       "      (embeddings): Idefics3VisionEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), padding=valid)\n",
       "        (position_embedding): Embedding(1024, 768)\n",
       "      )\n",
       "      (encoder): Idefics3Encoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x Idefics3EncoderLayer(\n",
       "            (self_attn): Idefics3VisionAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Idefics3VisionMLP(\n",
       "              (activation_fn): PytorchGELUTanh()\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (connector): Idefics3Connector(\n",
       "      (modality_projection): Idefics3SimpleMLP(\n",
       "        (proj): Linear(in_features=12288, out_features=576, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (text_model): LlamaModel(\n",
       "      (embed_tokens): Embedding(49280, 576, padding_idx=2)\n",
       "      (layers): ModuleList(\n",
       "        (0-29): 30 x LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "            (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "            (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "            (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "            (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "            (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "          (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=576, out_features=49280, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df8cfe64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1, 11126,    42,  1812,   314,  1029,    47, 49279,   198,  9519,\n",
      "          9531,    42]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([    1, 11126,    42,  1812,   314,  1029,    47, 49279,   198,  9519,\n",
       "         9531,    42,  5330,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30,   657,   314,   260,   768,  1070,  2775,   281,   451,\n",
       "          905,    30])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare inputs\n",
    "prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "# inputs = processor(text=prompt, images=[image1], return_tensors=\"pt\")\n",
    "inputs = processor(text=prompt, return_tensors=\"pt\")\n",
    "inputs = inputs.to(\"cpu\")\n",
    "\n",
    "print(inputs[\"input_ids\"])\n",
    "# Generate outputs\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=500,\n",
    "        # repition_penalty=1.1,  # Apply repeat penalty\n",
    "        output_scores=True,           # Return logits for each generated token\n",
    "        return_dict_in_generate=True, # Return detailed output object\n",
    "        do_sample=False,  # Use greedy decoding (highest logit)\n",
    "    )\n",
    "\n",
    "outputs.sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6ee5cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'User: What is life?\\nAssistant: Life is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world. It is the most important thing in this world.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09017432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved activations to ../../../tests/data/extracted_layer_activations.safetensors\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from safetensors.torch import save_file as save_safetensor\n",
    "\n",
    "# Specify the layer index you want to save activations for\n",
    "layer_idx = 0  # Change this to the desired layer index\n",
    "token_pos = 10  # Change this to the desired token position if needed\n",
    "\n",
    "# Keys for input and output activations in the embeddings dict\n",
    "input_key = f\"input_layernorm_d{layer_idx}_{token_pos}\"\n",
    "output_key = f\"self_attn_q_proj_d{layer_idx}_{token_pos}\"\n",
    "\n",
    "# Retrieve tensors from embeddings dict\n",
    "input_tensor = embeddings.get(input_key)\n",
    "output_tensor = embeddings.get(output_key)\n",
    "\n",
    "if input_tensor is None or output_tensor is None:\n",
    "    raise ValueError(f\"Could not find input or output tensor for layer {layer_idx} and token {token_pos}.\")\n",
    "\n",
    "# Prepare dict for safetensors\n",
    "to_save = {\n",
    "    \"input\": input_tensor,\n",
    "    \"output\": output_tensor,\n",
    "}\n",
    "\n",
    "# Save to safetensors file\n",
    "save_path = os.path.join(\"../validation_data\", f\"extracted_layer_activations.safetensors\")\n",
    "save_safetensor(to_save, save_path)\n",
    "print(f\"Saved activations to {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
